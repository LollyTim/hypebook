# Node Monitoring & Maintenance

Comprehensive guide to monitoring your HyperEVM node health, performance, and maintaining optimal operation. Essential for validators and infrastructure providers.

## Overview

Effective monitoring ensures:

- **High Availability**: Detect and resolve issues before they impact service
- **Performance Optimization**: Identify bottlenecks and optimize resource usage
- **Security**: Monitor for suspicious activity and potential attacks
- **Compliance**: Meet validator uptime requirements and SLA commitments

## Monitoring Stack Setup

### 1. Prometheus + Grafana Stack

The most comprehensive monitoring solution for HyperEVM nodes.

#### Install Prometheus

```bash
# Create monitoring directory
mkdir -p ~/monitoring/{prometheus,grafana,alertmanager}
cd ~/monitoring

# Create Prometheus configuration
cat > prometheus/prometheus.yml << EOF
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "hyperevm_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'hyperevm-node'
    static_configs:
      - targets: ['hyperevm-node:9090']
    scrape_interval: 10s
    metrics_path: /metrics

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
EOF

# Create alert rules
cat > prometheus/hyperevm_rules.yml << EOF
groups:
- name: hyperevm_alerts
  rules:
  - alert: HyperEVMNodeDown
    expr: up{job="hyperevm-node"} == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "HyperEVM node is down"
      description: "HyperEVM node has been down for more than 30 seconds"

  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 80% for more than 2 minutes"

  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is above 90% for more than 2 minutes"

  - alert: DiskSpaceLow
    expr: (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"})) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Disk space running low"
      description: "Disk usage is above 85%"

  - alert: HyperEVMSyncBehind
    expr: hyperevm_blockchain_head_block - hyperevm_blockchain_current_block > 10
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "HyperEVM node falling behind"
      description: "Node is more than 10 blocks behind the network"

  - alert: ValidatorMissedBlocks
    expr: increase(hyperevm_validator_missed_blocks[5m]) > 3
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Validator missing blocks"
      description: "Validator has missed more than 3 blocks in the last 5 minutes"
EOF
```

#### Docker Compose for Monitoring

```yaml
# monitoring/docker-compose.yml
version: "3.8"

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=your_secure_password
      - GF_USERS_ALLOW_SIGN_UP=false

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    restart: unless-stopped
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:
```

### 2. Configure Alertmanager

```bash
# Create alertmanager configuration
cat > alertmanager/alertmanager.yml << EOF
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@yourvalidator.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-app-password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
- name: 'web.hook'
  email_configs:
  - to: 'admin@yourvalidator.com'
    subject: 'HyperEVM Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Labels: {{ .Labels }}
      {{ end }}

  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#alerts'
    title: 'HyperEVM Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']
EOF
```

### 3. Start Monitoring Stack

```bash
cd ~/monitoring
docker-compose up -d

# Verify services are running
docker-compose ps

# Check Prometheus targets
curl http://localhost:9090/api/v1/targets

# Access Grafana at http://localhost:3000
# Default credentials: admin / your_secure_password
```

## Custom Metrics Collection

### 1. Enable Metrics in HyperEVM Node

Update your HyperEVM node configuration to expose metrics:

```toml
# config/node.toml
[metrics]
enabled = true
port = 9090
host = "0.0.0.0"
path = "/metrics"
interval = "10s"

[validator.metrics]
enabled = true
detailed = true
```

Update `docker-compose.yml` to expose metrics port:

```yaml
services:
  hyperevm-node:
    # ... existing config
    ports:
      - "8545:8545"
      - "8546:8546"
      - "30303:30303"
      - "9090:9090" # Metrics port
```

### 2. Custom Metrics Script

Create a script to collect additional metrics:

```python
#!/usr/bin/env python3
# custom_metrics.py

import requests
import json
import time
from prometheus_client import start_http_server, Gauge, Counter
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prometheus metrics
block_height = Gauge('hyperevm_block_height', 'Current block height')
peer_count = Gauge('hyperevm_peer_count', 'Number of connected peers')
gas_price = Gauge('hyperevm_gas_price', 'Current gas price in wei')
tx_pool_size = Gauge('hyperevm_tx_pool_size', 'Transaction pool size')
validator_balance = Gauge('hyperevm_validator_balance', 'Validator balance in HYPE')
validator_stake = Gauge('hyperevm_validator_stake', 'Total validator stake')

class HyperEVMMetrics:
    def __init__(self, rpc_url, validator_address=None):
        self.rpc_url = rpc_url
        self.validator_address = validator_address

    def make_rpc_call(self, method, params=None):
        payload = {
            "jsonrpc": "2.0",
            "method": method,
            "params": params or [],
            "id": 1
        }
        try:
            response = requests.post(self.rpc_url, json=payload, timeout=10)
            response.raise_for_status()
            return response.json().get('result')
        except Exception as e:
            logger.error(f"RPC call failed: {method} - {e}")
            return None

    def collect_metrics(self):
        # Block height
        block_number = self.make_rpc_call("eth_blockNumber")
        if block_number:
            block_height.set(int(block_number, 16))

        # Peer count
        peer_count_hex = self.make_rpc_call("net_peerCount")
        if peer_count_hex:
            peer_count.set(int(peer_count_hex, 16))

        # Gas price
        gas_price_hex = self.make_rpc_call("eth_gasPrice")
        if gas_price_hex:
            gas_price.set(int(gas_price_hex, 16))

        # Transaction pool
        tx_pool = self.make_rpc_call("txpool_status")
        if tx_pool:
            pending = int(tx_pool.get('pending', '0x0'), 16)
            queued = int(tx_pool.get('queued', '0x0'), 16)
            tx_pool_size.set(pending + queued)

        # Validator specific metrics
        if self.validator_address:
            balance = self.make_rpc_call("eth_getBalance", [self.validator_address, "latest"])
            if balance:
                validator_balance.set(int(balance, 16) / 10**18)  # Convert to HYPE

            # Get validator stake (custom RPC method)
            stake = self.make_rpc_call("validator_getStake", [self.validator_address])
            if stake:
                validator_stake.set(int(stake, 16) / 10**18)

def main():
    # Configuration
    RPC_URL = "http://localhost:8545"
    VALIDATOR_ADDRESS = "0x..."  # Your validator address
    METRICS_PORT = 8000
    COLLECTION_INTERVAL = 30  # seconds

    # Start Prometheus metrics server
    start_http_server(METRICS_PORT)
    logger.info(f"Metrics server started on port {METRICS_PORT}")

    # Initialize metrics collector
    collector = HyperEVMMetrics(RPC_URL, VALIDATOR_ADDRESS)

    # Collection loop
    while True:
        try:
            collector.collect_metrics()
            logger.info("Metrics collected successfully")
        except Exception as e:
            logger.error(f"Error collecting metrics: {e}")

        time.sleep(COLLECTION_INTERVAL)

if __name__ == "__main__":
    main()
```

Install and run the custom metrics collector:

```bash
# Install dependencies
pip3 install requests prometheus_client

# Run as service
python3 custom_metrics.py &

# Or create systemd service
sudo tee /etc/systemd/system/hyperevm-metrics.service << EOF
[Unit]
Description=HyperEVM Custom Metrics Collector
After=network.target

[Service]
Type=simple
User=hyperevm
WorkingDirectory=/home/hyperevm
ExecStart=/usr/bin/python3 /home/hyperevm/custom_metrics.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable hyperevm-metrics
sudo systemctl start hyperevm-metrics
```

## Grafana Dashboard Setup

### 1. Import HyperEVM Dashboard

Create a comprehensive Grafana dashboard:

```json
{
  "dashboard": {
    "id": null,
    "title": "HyperEVM Node Dashboard",
    "tags": ["hyperevm", "blockchain", "validator"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Node Status",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"hyperevm-node\"}",
            "legendFormat": "Node Status"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                { "color": "red", "value": 0 },
                { "color": "green", "value": 1 }
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Block Height",
        "type": "stat",
        "targets": [
          {
            "expr": "hyperevm_block_height",
            "legendFormat": "Current Block"
          }
        ]
      },
      {
        "id": 3,
        "title": "Peer Count",
        "type": "stat",
        "targets": [
          {
            "expr": "hyperevm_peer_count",
            "legendFormat": "Connected Peers"
          }
        ]
      },
      {
        "id": 4,
        "title": "CPU Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ]
      },
      {
        "id": 5,
        "title": "Memory Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "Memory Usage %"
          }
        ]
      },
      {
        "id": 6,
        "title": "Disk Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "(1 - (node_filesystem_avail_bytes{fstype!=\"tmpfs\"} / node_filesystem_size_bytes{fstype!=\"tmpfs\"})) * 100",
            "legendFormat": "Disk Usage %"
          }
        ]
      },
      {
        "id": 7,
        "title": "Network I/O",
        "type": "timeseries",
        "targets": [
          {
            "expr": "irate(node_network_receive_bytes_total[5m])",
            "legendFormat": "Received"
          },
          {
            "expr": "irate(node_network_transmit_bytes_total[5m])",
            "legendFormat": "Transmitted"
          }
        ]
      },
      {
        "id": 8,
        "title": "Transaction Pool",
        "type": "timeseries",
        "targets": [
          {
            "expr": "hyperevm_tx_pool_size",
            "legendFormat": "Pending Transactions"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s"
  }
}
```

### 2. Validator-Specific Dashboard

```json
{
  "dashboard": {
    "title": "HyperEVM Validator Dashboard",
    "panels": [
      {
        "id": 1,
        "title": "Validator Balance",
        "type": "stat",
        "targets": [
          {
            "expr": "hyperevm_validator_balance",
            "legendFormat": "HYPE Balance"
          }
        ]
      },
      {
        "id": 2,
        "title": "Total Stake",
        "type": "stat",
        "targets": [
          {
            "expr": "hyperevm_validator_stake",
            "legendFormat": "Total Stake"
          }
        ]
      },
      {
        "id": 3,
        "title": "Blocks Produced",
        "type": "timeseries",
        "targets": [
          {
            "expr": "increase(hyperevm_validator_blocks_produced[1h])",
            "legendFormat": "Blocks/Hour"
          }
        ]
      },
      {
        "id": 4,
        "title": "Missed Blocks",
        "type": "timeseries",
        "targets": [
          {
            "expr": "increase(hyperevm_validator_missed_blocks[1h])",
            "legendFormat": "Missed/Hour"
          }
        ]
      },
      {
        "id": 5,
        "title": "Validator Uptime",
        "type": "stat",
        "targets": [
          {
            "expr": "(1 - (hyperevm_validator_missed_blocks / hyperevm_validator_expected_blocks)) * 100",
            "legendFormat": "Uptime %"
          }
        ]
      }
    ]
  }
}
```

## Log Management

### 1. Centralized Logging with ELK Stack

```yaml
# logging/docker-compose.yml
version: "3.8"

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config:/usr/share/logstash/config
    ports:
      - "5000:5000"
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

### 2. Logstash Configuration

```ruby
# logstash/pipeline/hyperevm.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "hyperevm" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}" }
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    if [level] == "ERROR" {
      mutate {
        add_tag => ["error"]
      }
    }

    if [message] =~ /validator/ {
      mutate {
        add_tag => ["validator"]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "hyperevm-logs-%{+YYYY.MM.dd}"
  }
}
```

### 3. Log Rotation and Retention

```bash
# Create log rotation script
cat > /etc/logrotate.d/hyperevm << EOF
/home/hyperevm/hyperevm-node/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 hyperevm hyperevm
    postrotate
        docker-compose -f /home/hyperevm/hyperevm-node/docker-compose.yml restart hyperevm-node > /dev/null 2>&1 || true
    endscript
}
EOF

# Test log rotation
sudo logrotate -d /etc/logrotate.d/hyperevm
```

## Automated Health Checks

### 1. Comprehensive Health Check Script

```bash
#!/bin/bash
# health_check.sh

set -euo pipefail

# Configuration
RPC_URL="http://localhost:8545"
VALIDATOR_ADDRESS="0x..."
LOG_FILE="/var/log/hyperevm-health.log"
ALERT_EMAIL="admin@yourvalidator.com"
SLACK_WEBHOOK="https://hooks.slack.com/services/..."

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') $1" | tee -a "$LOG_FILE"
}

alert() {
    local message="$1"
    local severity="${2:-WARNING}"

    log "[$severity] $message"

    # Send email alert
    echo "$message" | mail -s "HyperEVM Alert: $severity" "$ALERT_EMAIL"

    # Send Slack alert
    curl -X POST -H 'Content-type: application/json' \
        --data "{\"text\":\"ðŸš¨ HyperEVM Alert: $message\"}" \
        "$SLACK_WEBHOOK" > /dev/null 2>&1
}

check_rpc() {
    local response
    response=$(curl -s -X POST -H "Content-Type: application/json" \
        --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}' \
        "$RPC_URL" || echo "ERROR")

    if [[ "$response" == "ERROR" ]] || [[ ! "$response" =~ \"result\" ]]; then
        alert "RPC endpoint is not responding" "CRITICAL"
        return 1
    fi

    log "âœ“ RPC endpoint is healthy"
    return 0
}

check_sync_status() {
    local sync_status
    sync_status=$(curl -s -X POST -H "Content-Type: application/json" \
        --data '{"jsonrpc":"2.0","method":"eth_syncing","params":[],"id":1}' \
        "$RPC_URL" | jq -r '.result')

    if [[ "$sync_status" != "false" ]]; then
        alert "Node is still syncing: $sync_status" "WARNING"
        return 1
    fi

    log "âœ“ Node is fully synced"
    return 0
}

check_peer_count() {
    local peer_count
    peer_count=$(curl -s -X POST -H "Content-Type: application/json" \
        --data '{"jsonrpc":"2.0","method":"net_peerCount","params":[],"id":1}' \
        "$RPC_URL" | jq -r '.result')

    peer_count=$(printf "%d" "$peer_count")

    if [[ $peer_count -lt 5 ]]; then
        alert "Low peer count: $peer_count" "WARNING"
        return 1
    fi

    log "âœ“ Peer count is healthy: $peer_count"
    return 0
}

check_validator_status() {
    if [[ -z "$VALIDATOR_ADDRESS" ]]; then
        return 0
    fi

    local is_active
    is_active=$(curl -s -X POST -H "Content-Type: application/json" \
        --data "{\"jsonrpc\":\"2.0\",\"method\":\"validator_isActive\",\"params\":[\"$VALIDATOR_ADDRESS\"],\"id\":1}" \
        "$RPC_URL" | jq -r '.result')

    if [[ "$is_active" != "true" ]]; then
        alert "Validator is not active" "CRITICAL"
        return 1
    fi

    log "âœ“ Validator is active"
    return 0
}

check_disk_space() {
    local usage
    usage=$(df /home/hyperevm/hyperevm-node/data | tail -1 | awk '{print $5}' | sed 's/%//')

    if [[ $usage -gt 90 ]]; then
        alert "Disk usage critical: ${usage}%" "CRITICAL"
        return 1
    elif [[ $usage -gt 80 ]]; then
        alert "Disk usage warning: ${usage}%" "WARNING"
    fi

    log "âœ“ Disk usage OK: ${usage}%"
    return 0
}

check_memory_usage() {
    local usage
    usage=$(free | grep Mem | awk '{printf "%.0f", $3/$2 * 100.0}')

    if [[ $usage -gt 95 ]]; then
        alert "Memory usage critical: ${usage}%" "CRITICAL"
        return 1
    elif [[ $usage -gt 85 ]]; then
        alert "Memory usage warning: ${usage}%" "WARNING"
    fi

    log "âœ“ Memory usage OK: ${usage}%"
    return 0
}

check_container_health() {
    local container_status
    container_status=$(docker-compose -f /home/hyperevm/hyperevm-node/docker-compose.yml ps -q hyperevm-node | xargs docker inspect --format='{{.State.Health.Status}}' 2>/dev/null || echo "unknown")

    if [[ "$container_status" != "healthy" ]] && [[ "$container_status" != "unknown" ]]; then
        alert "Container health check failed: $container_status" "CRITICAL"
        return 1
    fi

    log "âœ“ Container is healthy"
    return 0
}

main() {
    log "Starting health check..."

    local failed_checks=0

    check_rpc || ((failed_checks++))
    check_sync_status || ((failed_checks++))
    check_peer_count || ((failed_checks++))
    check_validator_status || ((failed_checks++))
    check_disk_space || ((failed_checks++))
    check_memory_usage || ((failed_checks++))
    check_container_health || ((failed_checks++))

    if [[ $failed_checks -eq 0 ]]; then
        log "âœ… All health checks passed"
    else
        log "âŒ $failed_checks health checks failed"
        exit 1
    fi
}

main "$@"
```

### 2. Automated Recovery Script

```bash
#!/bin/bash
# auto_recovery.sh

source /home/hyperevm/health_check.sh

recover_node() {
    log "Attempting automatic recovery..."

    # Restart the node
    cd /home/hyperevm/hyperevm-node
    docker-compose restart hyperevm-node

    # Wait for node to start
    sleep 30

    # Check if recovery was successful
    if check_rpc && check_sync_status; then
        log "âœ… Automatic recovery successful"
        alert "Node automatically recovered" "INFO"
        return 0
    else
        log "âŒ Automatic recovery failed"
        alert "Automatic recovery failed - manual intervention required" "CRITICAL"
        return 1
    fi
}

# Run health check and attempt recovery if needed
if ! main; then
    recover_node
fi
```

### 3. Cron Job Setup

```bash
# Install health check as cron job
crontab -e

# Add these lines:
# Run health check every 5 minutes
*/5 * * * * /home/hyperevm/health_check.sh >> /var/log/hyperevm-health.log 2>&1

# Run comprehensive check every hour
0 * * * * /home/hyperevm/comprehensive_check.sh >> /var/log/hyperevm-comprehensive.log 2>&1

# Daily backup and cleanup
0 2 * * * /home/hyperevm/backup.sh && /home/hyperevm/cleanup_logs.sh
```

## Performance Monitoring

### 1. Blockchain-Specific Metrics

```python
#!/usr/bin/env python3
# blockchain_metrics.py

import requests
import time
import statistics
from datetime import datetime, timedelta

class BlockchainMetrics:
    def __init__(self, rpc_url):
        self.rpc_url = rpc_url
        self.block_times = []
        self.gas_usage = []

    def get_block_info(self, block_number="latest"):
        payload = {
            "jsonrpc": "2.0",
            "method": "eth_getBlockByNumber",
            "params": [block_number, False],
            "id": 1
        }
        response = requests.post(self.rpc_url, json=payload)
        return response.json().get('result')

    def calculate_block_time(self):
        current_block = self.get_block_info()
        previous_block = self.get_block_info(hex(int(current_block['number'], 16) - 1))

        current_time = int(current_block['timestamp'], 16)
        previous_time = int(previous_block['timestamp'], 16)

        block_time = current_time - previous_time
        self.block_times.append(block_time)

        # Keep only last 100 block times
        if len(self.block_times) > 100:
            self.block_times.pop(0)

        return block_time

    def calculate_gas_usage(self):
        block = self.get_block_info()
        gas_used = int(block['gasUsed'], 16)
        gas_limit = int(block['gasLimit'], 16)

        usage_percent = (gas_used / gas_limit) * 100
        self.gas_usage.append(usage_percent)

        if len(self.gas_usage) > 100:
            self.gas_usage.pop(0)

        return usage_percent

    def get_stats(self):
        if not self.block_times:
            return {}

        return {
            'avg_block_time': statistics.mean(self.block_times),
            'median_block_time': statistics.median(self.block_times),
            'avg_gas_usage': statistics.mean(self.gas_usage) if self.gas_usage else 0,
            'median_gas_usage': statistics.median(self.gas_usage) if self.gas_usage else 0,
            'last_block_time': self.block_times[-1] if self.block_times else 0,
            'last_gas_usage': self.gas_usage[-1] if self.gas_usage else 0
        }

# Usage
metrics = BlockchainMetrics("http://localhost:8545")

while True:
    try:
        block_time = metrics.calculate_block_time()
        gas_usage = metrics.calculate_gas_usage()
        stats = metrics.get_stats()

        print(f"Block Time: {block_time}s, Gas Usage: {gas_usage:.1f}%")
        print(f"Avg Block Time: {stats['avg_block_time']:.1f}s")

    except Exception as e:
        print(f"Error: {e}")

    time.sleep(30)
```

## Alerting Best Practices

### 1. Alert Fatigue Prevention

```yaml
# Alert grouping and throttling
groups:
  - name: hyperevm_critical
    rules:
      - alert: NodeDown
        expr: up{job="hyperevm-node"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "HyperEVM node is down"
          runbook_url: "https://docs.yourvalidator.com/runbooks/node-down"

  - name: hyperevm_warning
    rules:
      - alert: HighResourceUsage
        expr: |
          (
            100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
          ) or (
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
          )
        for: 10m
        labels:
          severity: warning
          team: infrastructure
```

### 2. Escalation Policies

```yaml
# alertmanager.yml
route:
  group_by: ["severity", "team"]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: "default"
  routes:
    - match:
        severity: critical
      receiver: "critical-alerts"
      group_wait: 10s
      repeat_interval: 30m
    - match:
        severity: warning
      receiver: "warning-alerts"
      repeat_interval: 2h

receivers:
  - name: "critical-alerts"
    email_configs:
      - to: "oncall@yourvalidator.com"
        subject: "CRITICAL: {{ .GroupLabels.alertname }}"
    pagerduty_configs:
      - routing_key: "your-pagerduty-key"
        severity: "critical"

  - name: "warning-alerts"
    slack_configs:
      - api_url: "your-slack-webhook"
        channel: "#alerts"
        title: "Warning: {{ .GroupLabels.alertname }}"
```

## Maintenance Procedures

### 1. Regular Maintenance Tasks

```bash
#!/bin/bash
# maintenance.sh

# Weekly maintenance script
perform_weekly_maintenance() {
    log "Starting weekly maintenance..."

    # Update system packages
    sudo apt update && sudo apt upgrade -y

    # Clean up old logs
    find /var/log -name "*.log" -mtime +30 -delete

    # Optimize database
    docker exec hyperevm-node hyperevm-cli db optimize

    # Check and repair filesystem
    sudo fsck -f /dev/nvme0n1

    # Update Docker images
    docker-compose pull

    log "Weekly maintenance completed"
}

# Monthly maintenance
perform_monthly_maintenance() {
    log "Starting monthly maintenance..."

    # Full backup
    ./backup.sh --full

    # Analyze disk usage
    du -sh /home/hyperevm/hyperevm-node/data/*

    # Security updates
    sudo unattended-upgrade

    # Certificate renewal
    sudo certbot renew

    log "Monthly maintenance completed"
}
```

### 2. Emergency Procedures

```bash
#!/bin/bash
# emergency_procedures.sh

emergency_shutdown() {
    log "EMERGENCY: Initiating graceful shutdown"

    # Stop accepting new connections
    sudo iptables -A INPUT -p tcp --dport 8545 -j DROP

    # Gracefully stop validator
    docker-compose stop hyperevm-node

    # Create emergency backup
    tar -czf /backup/emergency-$(date +%s).tar.gz ~/hyperevm-node/data

    alert "Emergency shutdown completed" "CRITICAL"
}

emergency_recovery() {
    log "Starting emergency recovery..."

    # Restore from latest backup
    ./restore_backup.sh --latest

    # Start node
    docker-compose up -d

    # Remove firewall block
    sudo iptables -D INPUT -p tcp --dport 8545 -j DROP

    # Verify recovery
    sleep 60
    ./health_check.sh

    alert "Emergency recovery completed" "INFO"
}
```

## Troubleshooting Common Issues

### 1. High Resource Usage

```bash
# Identify resource bottlenecks
htop
iostat -x 1 10
iotop -o

# Check container resource usage
docker stats hyperevm-node

# Optimize if needed
# Increase cache sizes in node.toml
[cache]
database = 4096
trie = 2048
snapshot = 1024
```

### 2. Network Issues

```bash
# Check network connectivity
ping -c 10 bootnode1.hyperliquid.xyz
traceroute bootnode1.hyperliquid.xyz
netstat -tuln | grep :30303

# Monitor network traffic
iftop -i eth0
```

### 3. Database Corruption

```bash
# Check database integrity
docker exec hyperevm-node hyperevm-cli db check

# Repair if needed
docker-compose stop hyperevm-node
docker exec hyperevm-node hyperevm-cli db repair
docker-compose start hyperevm-node
```

## Next Steps

After implementing comprehensive monitoring:

1. **Set up redundancy**: Implement failover systems for critical infrastructure
2. **Automate scaling**: Create scripts to handle increased load automatically
3. **Performance tuning**: Use monitoring data to optimize node performance
4. **Capacity planning**: Predict future resource needs based on growth trends
5. **Disaster recovery**: Test and refine your disaster recovery procedures

For advanced infrastructure topics, continue to our [Performance Optimization](/docs/infrastructure/optimization) guide.

## Support Resources

- **Monitoring Tools**: [Grafana Dashboards](https://grafana.com/grafana/dashboards)
- **Prometheus Exporters**: [Official Exporters](https://prometheus.io/docs/instrumenting/exporters/)
- **Community**: [HyperEVM Operators Discord](https://discord.gg/hyperliquid)
- **Professional Support**: Contact us for enterprise monitoring solutions

